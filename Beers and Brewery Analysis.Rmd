---
title: "Beers and Brewery Project"
author: "Renfeng Wang"
date: "10/14/2020"
output: html_document
---

```{r}
library(class)
library(caret)
library(e1071)
library(magrittr)
library(XML) 
library(dplyr)
library(tidyr)
library(stringi)
library(rvest) 
library(ggplot2)
library(RCurl) 
library(plotly)
library(gridExtra)
library(httr)
library(jsonlite)
library(e1071)
library(naniar)
library(scales)
library(gghighlight)
library(stringr)
library(knitr)
library(mice)
library(Hmisc)
library(ggpubr)
library(maps)
library(usmap)
knitr::opts_chunk$set(error = TRUE)
beers=read.csv('/Users/renfengwang/Documents/SMU\ Data\ Science\ Program/Doing\ Data\ Science/Project\ 1/Beers.csv',header=T)
brewery=read.csv('/Users/renfengwang/Documents/SMU\ Data\ Science\ Program/Doing\ Data\ Science/Project\ 1/Breweries.csv',header=T)
head(beers)
head(brewery)

```
Load both CSV files and printed out the head of each data frame to check them visually.
```{r}
#Question 1
brewery %>% arrange(State) %>% ggplot(aes(x=State),count=Name)+geom_bar()+geom_text(aes(label=..count..),stat='count',vjust=-.5) +
  xlab('States')+ ylab('Brewery Numbers') + ggtitle('Numbers of Brewery by State')  #Bar plot to count the brewery numbers in each state

```
Plot the numbers of brewery by state.
Colorado (47) has the most breweries.
California (39), Michigan (32), Oregon (29) and Texas (28) are the next top four states with max breweries.
```{r}
#Question 2
colnames(beers)[colnames(beers)=='Brewery_id']='Brew_ID'  #Change one of the data frame column names before merging two data sets.
df_beer=merge(beers, brewery, by='Brew_ID', all=T)  #Outer join two data frames.
colnames(df_beer)[colnames(df_beer)=='Name.x']='Beer_Name'
colnames(df_beer)[colnames(df_beer)=='Name.y']='Brewery_Name'

head(df_beer) 
tail(df_beer)

```
Merged two data frames and printed out head and tail of the new combined data frame.
```{r}
#Question 3

df_beerall=df_beer %>% replace_with_na_all(condition = ~.x=='')
df_beerall=df_beerall %>% replace_with_na_all(condition = ~.x==' ') # Replace any blank values with NA
gg_miss_var(df_beerall) #Plot to check if any column has missing values

table(is.na(df_beerall$IBU))
table(is.na(df_beerall$ABV))
table(is.na(df_beerall$ABV | df_beerall$IBU)) #Printed out the numbers of missing values in ABV and IBU columns

imp=mice(df_beerall, method='norm.predict', m=5)
beer_imp=complete(imp) #Use linear regression method to replace missing values in ABV and IBU columns.
gg_miss_var(beer_imp)
table(is.na(beer_imp$IBU))
table(is.na(beer_imp$ABV))
table(is.na(beer_imp$Style)) #Check how many missing values in beer style column
```
We can see there are missing values in Style, ABV and IBU columns.
In ABV column, there are 62 missing values.
In IBU column, there are 1005 missing values.
In Style column, there are 5 missing values.
I replaced the missing values in both ABV and IBU columns by linear regression method.
I will replace the missing values in Beer Style column by searching their styles online as there are only 5 missing values.
```{r}
#Question 4
Median_ABV_IBU=beer_imp %>% arrange(State) %>% group_by(State) %>% summarise(Median_ABV=median(ABV, na.rm=TRUE), Median_IBU=median(IBU,na.rm=TRUE))

Median_ABV_IBU %>% ggplot(aes(x=State, y=Median_ABV,width=.5))+geom_bar(stat='identity') + 
  geom_text(aes(label=percent(Median_ABV, accuracy = 0.01)),vjust=-.5,size=2.5,check_overlap = T) +
  xlab('States') +ylab('Median Alcoholic Content')+ggtitle('Median Alcoholic Content by State')

Median_ABV_IBU %>% ggplot(aes(x=State, y=Median_IBU, width=.5))+geom_bar(stat='identity') + 
  geom_text(aes(label=sprintf("%0.1f", round(Median_IBU, digits = 1))),vjust=-.5,size=2.5,check_overlap = T) +
  xlab('States') +ylab('Median International Bitterness')+ggtitle('Median International Bitterness by State') 
```
Plot median ABV and median IBU by each state.
Washington DC and Kentucky have the highest median ABV values which are 6.25%.
Maine has the highest median IBU value 61 followed by West Virginia 57.5.
```{r}
#Question 5
Max_ABV_IBU=beer_imp %>% arrange(State) %>% group_by(State) %>% summarise(Max_ABV=max(ABV, na.rm=TRUE), Max_IBU=max(IBU,na.rm=T))
Max_ABV_IBU %>% ggplot(aes(x=State, y=Max_ABV,width=.5))+geom_bar(stat='identity') + 
  geom_text(aes(label=percent(Max_ABV, accuracy = 0.01)),vjust=-.5,size=2.5,check_overlap = T) +
  xlab('States') +ylab('Max Alcoholic Content')+ggtitle('Max Alcoholic Content by State')

Max_ABV_IBU %>% ggplot(aes(x=State, y=Max_IBU,width=.5))+geom_bar(stat='identity') + 
  geom_text(aes(label=sprintf("%0.1f", round(Max_IBU, digits = 1))),vjust=-.5,size=2.5,check_overlap = T) +
  xlab('States') +ylab('Max International Bitterness')+ggtitle('Max International Bitterness by State')
```
The State with the maximum ABV beer is Colorado (12.8%). This is Upslope Brewing Company’s Lee Hill Series Vol.5- Belgian Style Quadrupel Ale.
The State with the maximum IBU beer is Oregon (138 IBU). This is Astoria Brewing Company’s Bitter Bitch Imperial IPA.
```{r}

#Question 6
beer_imp%>% summarise(Mean=mean(ABV, na.rm=TRUE),
                                             Median=median(ABV,na.rm=T),
                                             Min=min(ABV,na.rm=T),
                                             Max=max(ABV,na.rm=T),
                                             SD=sd(ABV,na.rm=T),
                                             N=n())

beer_imp %>% filter(!is.na(ABV)) %>% ggplot(aes(x=ABV))+geom_histogram(aes(y=..density..),colour='black',fill='white')+
  geom_density(alpha=.5, fill='#FF6666') #Plot the distribution of ABV
```
The distribution of ABV is right skewed. ABV of beers around 5% has the most counts. 
There are total 2410 non-missing ABV values in this data set. The maximum ABV is 12.8%, the minimum ABV is .1%, the median ABV is 5.6%.
The mean ABV is 5.98% and standard deviation of ABV is 1.35%.

```{r}
#Question 7
beer_imp %>% filter(!is.na(ABV) &!is.na(IBU)) %>% 
  ggplot(aes(y=ABV, x=IBU))+geom_point(position='jitter')+geom_smooth(method=loess)

rcorr(beer_imp$ABV, beer_imp$IBU,type='pearson')
ggscatter(beer_imp,x='IBU', y='ABV',add='reg.line',conf.int=T,cor.coef = T,cor.method='pearson')
#Checked the linear correlation between ABV and IBU

```
Most beers with lower IBU (less than 50) have ABV values around 5%. When IBU value increases, ABV values spreads out. But most beers with IBU values above 50, their ABV values spread out within the region between 5% and 10%.
We can find out that some dots have clearly linear regression as those are missing values which were replaced by linear regression method.
Thus, the correlation coefficient is 0.76 which is probably larger than if I remove all missing values.

```{r}
#Question 8  KNN
beer_imp %>% filter(is.na(beer_imp$Style)) #Print out all rows contains missing values in beer style column.

beer_imp$Style[beer_imp$Beer_ID=='2210']='Red Ale - American Amber/Red'
beer_imp$Style[beer_imp$Beer_ID=='2527']='Lager - Märzen/Oktoberfest'
beer_imp$Style[beer_imp$Beer_ID=='1635']='Scottish Ale'
beer_imp$Style[beer_imp$Beer_ID=='1796']='IPA - AMERICAN'
beer_imp$Style[beer_imp$Beer_ID=='1790']='FRUITED ALE'
#Replaced five missing values in beer style column to real data found online.   
           
df_beer_IPA=beer_imp %>% filter(!is.na(ABV) &!is.na(IBU)) %>% 
  filter(str_detect(Style, regex(str_c('\\b','IPA','\\b',sep=''), ignore_case = T))) #Find all IPA style beer

df_beer_IPA$Style=as.factor('IPA') #Replace all IPA style beer with style name 'IPA'

df_beer_Ale=beer_imp %>% filter(!is.na(ABV) &!is.na(IBU)) %>% 
  filter(str_detect(Style, regex(str_c('\\b','Ale','\\b',sep=''), ignore_case = T))) #Find all Ale style beer

df_beer_Ale$Style=as.factor('Ale') #Replace all Ale style beer with style name 'Ale'

df_beer_test=rbind(df_beer_IPA, df_beer_Ale) #Combine these two data frames

df_beer_test %>% ggplot(aes(x=ABV, y=IBU)) +geom_point(aes(colour=Style)) #Scatter plot for all IPA and Ale styles beer

iterations = 500
numks = 30
splitPerc = .7
masterAcc = matrix(nrow = iterations, ncol = numks)
set.seed(6)
trainIndices = sample(1:dim(df_beer_test)[1],round(splitPerc * dim(df_beer_test)[1])) #Shuffle the training and testing groups
beer_train = df_beer_test[trainIndices,]
beer_test = df_beer_test[-trainIndices,]
classifications=knn(beer_train[,c(4,5)],beer_test[,c(4,5)],beer_train$Style, prob = TRUE, k = 5) 
CM = confusionMatrix(table(classifications,beer_test$Style))
classifications  #Use KNN method to predict beer style by its ABV and IBU values
CM 
```
I replaced five missing values in beer style column missing by real beer style I got online.
I randomly assigned 70% of original data to training data and 30% of the original data as testing data when seed value is set to 6.
We can tell when k=5, the accuracy is about 79%.
```{r}
for(j in 1:iterations)
{
  accs = data.frame(accuracy = numeric(30), k = numeric(30))
  trainIndices = sample(1:dim(df_beer_test)[1],round(splitPerc * dim(df_beer_test)[1]))
  beer_train = df_beer_test[trainIndices,]
  beer_test = df_beer_test[-trainIndices,]
  for(i in 1:numks)
  {
    classifications = knn(beer_train[,c(4,5)],beer_test[,c(4,5)],beer_train$Style, prob = TRUE, k = i)
    table(classifications,beer_test$Style)
    CM = confusionMatrix(table(classifications,beer_test$Style))
    masterAcc[j,i] = CM$overall[1]
  }
}
MeanAcc = colMeans(masterAcc)
p=ggplot(mapping=aes(x=seq(1,numks,1), y=MeanAcc))+geom_line()
ggplotly(p)  #Shuffle the training and testing groups 500 times and loop the k value from 1 to 30 
```
I shuffled the training and testing data 500 times by 70%/30% split. And assigned integer k value from 1 to 30. 
From the plot, we can tell when k=5, it gives us the highest accuracy 85% which means we can predict the beer is either India Pale Ales or
any other types of Ale by knowing its ABV and IBU values with 85% accuracy when sets nearest neighbor numbers equals to 5
```{r}

#Naive Bayes--------------

iterations = 500
masterAcc = matrix(nrow = iterations)
masterSen = matrix(nrow = iterations)
masterSpec = matrix(nrow = iterations)
splitPerc = .7 
for(j in 1:iterations)
{
  
  trainIndices = sample(1:dim(df_beer_test)[1],round(splitPerc * dim(df_beer_test)[1]))
  beer_train = df_beer_test[trainIndices,]
  beer_test = df_beer_test[-trainIndices,]
  model = naiveBayes(beer_train[,c(4,5)],as.factor(beer_train$Style),laplace = 1)
  table(predict(model,beer_test[,c(4,5)]),as.factor(beer_test$Style))
  CM = confusionMatrix(table(predict(model,beer_test[,c(4,5)]),as.factor(beer_test$Style)))
  masterAcc[j] = CM$overall[1]
  masterSen[j] = CM$byClass[1]
  masterSpec[j] = CM$byClass[2]
}
MeanAcc = colMeans(masterAcc)
MeanSen = colMeans(masterSen)
MeanSpec = colMeans(masterSpec)
MeanAcc
MeanSen
MeanSpec  #Use Naive Bayes model to predict the mean accuracy by shufftling training and testing groups 500 times
```
Now I chose to use Naive Bayes model as I want to compare the accuracy with KNN model. I shuffled the training and testing data 500 times by 70%/30% split like I did in KNN model. The mean accuracy of Naive Bayes is 84% which is similar to what we got from KNN.
```{r}
#Question 9 ABV and IBU average by state
Mean_ABV_IBU=beer_imp %>% group_by(State) %>% summarise(Mean_ABV=mean(ABV, na.rm=TRUE), Mean_IBU=mean(IBU,na.rm=TRUE))
colnames(Mean_ABV_IBU)[colnames(Mean_ABV_IBU)=='State']='abbr'
statedata= statepop[,c(2,3)]
statedata$abbr=as.factor(statedata$abbr)
Mean_ABV_IBU$abbr=as.factor(Mean_ABV_IBU$abbr)
statedata1=merge(statedata,Mean_ABV_IBU,by='abbr',all.x=T)
statedata2=cbind(statedata1,Mean_ABV_IBU)
statedata3=statedata2[,c(1,2,6,7)]
colnames(statedata3)[colnames(statedata3)=='full']='state'  #Change the column name to state before plotting.

plot_usmap(data = statedata3, values = "Mean_ABV", color = "red") + 
  scale_fill_continuous(
    low = "white", high = "red", name = "Mean_ABV", label = scales::comma
  ) + theme(legend.position = "right") + ggtitle('Mean ABV by State')  #Plot average ABV by state

plot_usmap(data = statedata3, values = "Mean_IBU", color = "black") + 
  scale_fill_continuous(
    low = "white", high = "blue", name = "Mean_IBU", label = scales::comma
  ) + theme(legend.position = "right") + ggtitle('Mean IBU by State') #Plot average IBU by state
```
If the states' colors are close to white, it means lower mean IBU and ABV.

```{r}

#Back to Question 7
df_beer_study=rbind(df_beer_sort, df_beer_Ale)
df_beer_study %>% ggplot(aes(y=ABV, x=IBU))+geom_point(position='jitter')+geom_smooth() +facet_wrap(~Style)
```
  Now we came back to Question 7 by checking four different styles of beer. 
  We can see, majority of lager style beer has low IBU and its ABV values are around 5%. No ABV values of lager beer are above 7.5%.
  ABV values of Indian Pale Ale style seems like increases when IBU increases, but their ABV values don't pass 10% and majority of their ABV values are between 5% to 10%.
  The other types of Ale beer have low IBU values which most of them are below 50. Also, most of their ABV values are between 3.75% to 10%.
  The data size of stout style beer is small, but we can roughly tell their ABV increases when IBU increases.
